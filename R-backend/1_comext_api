# Easy Comext Bulk Data Downloader and Processor
# Description: This script downloads, extracts, and processes bulk trade data from the Easy Comext API.
# WARNING: Running this script may take up to few hours of downloading and processing. 

# Clean environment
rm(list = ls())
if (!is.null(dev.list())) dev.off()
cat("\014")  # Clear console

# Load required libraries
library(httr)
library(readr)
library(haven)
library(archive)
library(datasets)
library(dplyr)
library(arrow)
library(crayon)

# Set working directory and create output folder
output_dir <- "temp_comext_files"
dir.create(output_dir, showWarnings = FALSE)

# Generate list of YYYYMM dates from 2010 to September 2024. Change parameters of years and months for desired data.
months <- sprintf("%02d", 1:12)
years <- 2010:2024
dates <- unlist(lapply(years, function(y) paste0(y, months)))
dates <- dates[dates <= "202409"]

# Define base URL for downloading
base_url <- "https://ec.europa.eu/eurostat/api/dissemination/files/?sort=1&file=comext%2FCOMEXT_DATA%2FPRODUCTS%2F"

# Download .7z files for each date
for (date in dates) {
  file <- paste0("full", date, ".7z")
  url <- paste0(base_url, file)
  download_path <- file.path(output_dir, file)
  options(timeout = 500)
  GET(url, write_disk(download_path, overwrite = TRUE))
  cat("Downloaded:", date, "\n")
}

# Extract and load data into a list
final_data <- list()
for (date in dates) {
  file <- paste0("full", date, ".7z")
  download_path <- file.path(output_dir, file)
  extracted_files <- archive_extract(download_path, dir = output_dir)
  data <- read_delim(file.path(output_dir, extracted_files), delim = ",", col_names = TRUE)
  final_data[[date]] <- data
  cat("Extracted:", date, "\n")
}

# Merge all data into one data frame
merged_data <- bind_rows(final_data)

# Save full dataset to Parquet
write_parquet(merged_data, "full_data.parquet")

# Load essential columns only
essential_data <- read_parquet("full_data.parquet", col_select = c(
  "DECLARANT", "DECLARANT_ISO", "PARTNER_ISO", "TRADE_TYPE",
  "PRODUCT_NC", "FLOW", "PERIOD", "VALUE_IN_EUROS"
))

# Function to process a segment of dates and save to Parquet
process_dates <- function(dates_segment, output_file) {
  segment_data <- list()
  for (date in dates_segment) {
    extracted_file <- paste0("full", date, ".dat")
    data <- read_delim(
      file.path(output_dir, extracted_file),
      delim = ",",
      col_names = TRUE,
      col_types = cols(PRODUCT_BEC = col_character())  # Force column type
    )
    segment_data[[date]] <- data
    cat(green("Loaded:"), date, "\n")
    gc()  # Free memory
  }
  segment_data_df <- bind_rows(segment_data)
  write_parquet(segment_data_df, output_file)
  rm(segment_data, segment_data_df)
  gc()
  cat("Segment saved to:", output_file, "\n")
}

# Split dates into yearly segments
segments <- split(dates, ceiling(seq_along(dates) / 12))

# Process each segment and save to separate Parquet files
for (i in seq_along(segments)) {
  output_file <- file.path(output_dir, paste0("segment_", i, ".parquet"))
  process_dates(segments[[i]], output_file)
}

# Load and merge all segment files
parquet_files <- list.files(output_dir, pattern = "segment_.*\\.parquet$", full.names = TRUE)
parquet_files <- parquet_files[order(as.numeric(gsub(".*segment_(\\d+)\\.parquet$", "\\1", parquet_files)))]

merged_data <- lapply(parquet_files, function(file) {
  cat("Reading file:", file, "\n")
  read_parquet(file)
}) %>%
  bind_rows()

# Save final merged dataset
final_output <- file.path(output_dir, "full_data.parquet")
write_parquet(merged_data, final_output)
cat("All data saved to:", final_output, "\n")
